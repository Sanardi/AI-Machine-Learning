{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install guizero\n",
    "#%pip install guizero[images]\n",
    "#%pip install beautifulsoup4 --upgrade\n",
    "#%pip install JarvisAI\n",
    "#%load_ext Cython\n",
    "#%pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import cProfile\n",
    "#import pstats\n",
    "\n",
    "\n",
    "#cProfile.run('transformers.pipeline()', 'restats')\n",
    "#p = pstats.Stats('restats')\n",
    "#p.sort_stats('cumulative').print_stats(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport pickle\\nwith open('skype_chat.pkl', 'rb') as f:\\n    context = pickle.load(f)\\ncontext\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import pickle\n",
    "with open('skype_chat.pkl', 'rb') as f:\n",
    "    context = pickle.load(f)\n",
    "context\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "A = np.random.randn(4,3)\n",
    "B = np.sum(A, axis = 1, keepdims = True)\n",
    "B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mum2 = \" i love you but you know that you have to eat your dinner when you come home from school and please eat the healthy food that i put on the table and when you're done eating have some apple or have some food from the fridge oh you cannot play on the phone and don't hurt each other please don't fight play nicely okay mommy loves you but i'm please please stop leaving your plates on the table when you finish eating put your plate away you cannot play on the phone. you cannot play on the phone, go read a book. You cannot eat Chocolate but you can eat some fruitokay children you need to tidy up your dishes at yes i love you. school is great. i'm glad you love school and it's very cold make sure you properly dress and this is a long time one minute is a long time what else my children don't eat chocolate chocolate is bad for you eat something healthy or you could tidy up you cannot play on the phone, go read a book.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" i love you but you know that you have to eat your dinner when you come home from school and please eat the healthy food that i put on the table and when you're done eating have some apple or have some food from the fridge oh you cannot play on the phone and don't hurt each other please don't fight play nicely okay mommy loves you but i'm please please stop leaving your plates on the table when you finish eating put your plate away you cannot play on the phone. you cannot play on the phone, go read a book. You cannot eat Chocolate but you can eat some fruitokay children you need to tidy up your dishes at yes i love you. school is great. i'm glad you love school and it's very cold make sure you properly dress and this is a long time one minute is a long time what else my children don't eat chocolate chocolate is bad for you eat something healthy or you could tidy up you cannot play on the phone, go read a book. You cannot watch TV. You cannot watch TV. Felipe is a badass programmer\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pickle\n",
    "\n",
    "#with open('mummy.pkl', 'wb') as f:\n",
    "    #pickle.dump(mum2, f)\n",
    "\n",
    "with open('mummy.pkl', 'rb') as f:\n",
    "    mum2 = pickle.load(f)\n",
    "    \n",
    "#mum2 = mum2 + \" you cannot play on the phone, go read a book. You cannot eat Chocolate but you can eat some fruit\"\n",
    "mum2 = mum2 + \" You cannot watch TV. Felipe is a badass programmer\"\n",
    "mum2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use this to record a new message to add to AI mummy's rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Say something...\n",
      "You said: muhammad is bored\n",
      "\n",
      "muhammad is bored\n",
      "Say something...\n",
      "....\n",
      "Say something...\n",
      "You said: so let's go i am going to make her into a flask if i'm not keeping her like this\n",
      "\n",
      "so let's go i am going to make her into a flask if i'm not keeping her like this\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForQuestionAnswering\n",
    "from transformers.pipelines import pipeline\n",
    "import JarvisAI\n",
    "import re\n",
    "import pprint\n",
    "import random\n",
    "\n",
    "import time\n",
    "timeout = time.time() + 10   # 1 minute from now\n",
    "\n",
    "obj = JarvisAI.JarvisAssistant()\n",
    "\n",
    "def t2s(text):\n",
    "    obj.text2speech(text)\n",
    "\n",
    "mum = []\n",
    "\n",
    "while True:\n",
    "    \n",
    "    if time.time() > timeout:\n",
    "        break\n",
    "        \n",
    "    res = obj.mic_input()\n",
    "    \n",
    "   \n",
    "    print(res)\n",
    "\n",
    "    mum.append(res)\n",
    "\n",
    "    \n",
    "#mum.append(\". you cannot play on the phone, go read a book.\")\n",
    "mum2 = mum2 + \" \".join(mum) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" i love you but you know that you have to eat your dinner when you come home from school and please eat the healthy food that i put on the table and when you're done eating have some apple or have some food from the fridge oh you cannot play on the phone and don't hurt each other please don't fight play nicely okay mommy loves you but i'm please please stop leaving your plates on the table when you finish eating put your plate away you cannot play on the phone. you cannot play on the phone, go read a book. You cannot eat Chocolate but you can eat some fruitokay children you need to tidy up your dishes at yes i love you. school is great. i'm glad you love school and it's very cold make sure you properly dress and this is a long time one minute is a long time what else my children don't eat chocolate chocolate is bad for you eat something healthy or you could tidy up you cannot play on the phone, go read a book. You cannot watch TV. You cannot watch TV. Felipe is a badass programmer\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mum2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer \n",
    "# import SentimentIntensityAnalyzer class \n",
    "\n",
    "\n",
    "# function to print sentiments \n",
    "# of the sentence. \n",
    "def sentiment_scores(sentence): \n",
    "\n",
    "    # Create a SentimentIntensityAnalyzer object. \n",
    "    sid_obj = SentimentIntensityAnalyzer() \n",
    "\n",
    "    # polarity_scores method of SentimentIntensityAnalyzer \n",
    "    # oject gives a sentiment dictionary. \n",
    "    # which contains pos, neg, neu, and compound scores. \n",
    "    sentiment_dict = sid_obj.polarity_scores(sentence) \n",
    "    \n",
    "        \n",
    "    return sentiment_dict\n",
    "\n",
    "def sentiment(sentence): \n",
    "\n",
    "    # Create a SentimentIntensityAnalyzer object. \n",
    "    sid_obj = SentimentIntensityAnalyzer() \n",
    "\n",
    "    # polarity_scores method of SentimentIntensityAnalyzer \n",
    "    # oject gives a sentiment dictionary. \n",
    "    # which contains pos, neg, neu, and compound scores. \n",
    "    sentiment_dict = sid_obj.polarity_scores(sentence) \n",
    "    \n",
    "    print(\"Sentence Overall Rated As\",sentiment_dict, \"for\", sentence, end = \" \") \n",
    "\n",
    "    # decide sentiment as positive, negative and neutral \n",
    "    if sentiment_dict['compound'] >= 0.05 : \n",
    "        sentiment = \" Yaaaaay, super \"\n",
    "        pic = \"data\\mama_happy.jpeg\"\n",
    "\n",
    "    elif sentiment_dict['compound'] <= - 0.05 : \n",
    "        sentiment = \"Oh honey, that's too bad, my poor baby \"\n",
    "        pic = \"data\\mama_sad.jpeg\"\n",
    "        \n",
    "    elif 0.05 >  sentiment_dict['compound'] > - 0.05 : \n",
    "        sentiment = \"Hmmm, interesting. \"\n",
    "        pic = \"data\\mama.jpeg\"\n",
    "\n",
    "    else : \n",
    "        sentiment = my_name.value + \", My Sweetie. How can I help you?\"\n",
    "        pic = \"data\\mama_happy.jpeg\"\n",
    "        \n",
    "    return sentiment, pic\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer,AutoModelForQuestionAnswering\n",
    "from transformers.pipelines import pipeline\n",
    "import JarvisAI\n",
    "import re\n",
    "import pprint\n",
    "import random\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Allocate a pipeline for question-answering\n",
    "question_answerer = pipeline('question-answering')\n",
    "question_answerer({'question': 'can i play on the phone?','context': mum2})\n",
    "\n",
    "def answer(question):\n",
    "    return question_answerer({'question': question,'context': mum2})['answer']\n",
    "\n",
    "\n",
    "import time\n",
    "#timeout = time.time() + 60   # 1 minute from now\n",
    "#from guizero import App\n",
    "from guizero import App, Text, TextBox, PushButton, Picture, Window\n",
    "\n",
    "#########################################\n",
    "\n",
    "app = App(title=\"AI Mama\")\n",
    "\n",
    "########################################\n",
    "\n",
    "obj = JarvisAI.JarvisAssistant()\n",
    "\n",
    "def t2s(text):\n",
    "    obj.text2speech(text)\n",
    "\n",
    "import time\n",
    "  # 1 minute from now\n",
    "\n",
    "def kid(seconds):\n",
    "    timeout = time.time() + seconds\n",
    "    kid = []\n",
    "\n",
    "    while True:\n",
    "    \n",
    "        if time.time() > timeout:\n",
    "            break\n",
    "            #return kid_string\n",
    "        \n",
    "        nag = obj.mic_input()\n",
    "    \n",
    "   \n",
    "        print(nag)\n",
    "\n",
    "        kid.append(nag)\n",
    "        kid_string = \" \".join(kid)\n",
    "    if len(kid_string) == 0:\n",
    "        kid_string = \"My Cutie\"\n",
    "    return kid_string\n",
    "    \n",
    "\n",
    " \n",
    "   \n",
    "    \n",
    "def say_my_name():\n",
    "    #my_name.destroy()\n",
    "    update_text.destroy()\n",
    "    my_name = kid(10)\n",
    "    welcome_message.value = my_name.value + \", My Sweetie. I love you. Precious little cutie!\"\n",
    "    \n",
    "    choice1 = PushButton(app, text=\"I want to watch TV\", pady=20,\n",
    "    padx=10, command = say_yes)\n",
    "    choice2 = PushButton(app, text=\"I want to play on the phone\", pady=20,\n",
    "    padx=10, command = say_no)\n",
    "    choice3 = PushButton(app, text=\"I want to eat chocolate spread\", pady=20,\n",
    "    padx=10, command = say_yes)\n",
    "    \n",
    "\n",
    "def my_name():\n",
    "    #my_name.destroy()\n",
    "    my_name = kid(2)\n",
    "    get_name.destroy()\n",
    "    \n",
    "    if len(my_name) > 0:\n",
    "        my_name = my_name.split(\" \")[-1]\n",
    "        welcome_message.value = my_name + \", My Sweetie. Precious little cutie. How are you. How can I help you\"\n",
    "        t2s(welcome_message.value)\n",
    "    else: \n",
    "        welcome_message.value = \"My Sweetie. How can I help you?\"\n",
    "        t2s(welcome_message.value)\n",
    "    \n",
    "\n",
    "    \n",
    "def submit():\n",
    "    #sentence = nag.value\n",
    "    sentence = kid(8)\n",
    "    print(sentence)\n",
    "    sentiment_response, pic = sentiment(sentence)\n",
    "    if len(sentence) > 0:\n",
    "        mums_answer = answer(sentence)\n",
    "        print(mums_answer)\n",
    "        welcome_message.value = sentiment_response + mums_answer\n",
    "        t2s(welcome_message.value)\n",
    "        \n",
    "        picture1 = Picture(window, image=pic)\n",
    "        #window.hide()\n",
    "    else:\n",
    "        welcome_message.value = \"I did not understand, can you say repeat this?\"\n",
    "        t2s(welcome_message.value)\n",
    "        #my_pic = Picture(app, image=pic)\n",
    "    \n",
    "def say_yes():\n",
    "    welcome_message.value = \"OK, my cutie pie\"\n",
    "    \n",
    "def say_no():\n",
    "    welcome_message.value = \"Forget it.... and don't ask again!\"\n",
    "    \n",
    "\n",
    "\n",
    "window = Window(app, title=\"2nd window\")\n",
    " \n",
    "def open_window():\n",
    "    window.show(wait=True)\n",
    "\n",
    "my_pic = Picture(app, image=\"data\\mama.jpeg\")\n",
    "\n",
    "welcome_message = Text(app, text=\"Hi, Its me :-). AI Mama. Who are you?\", size=40, font=\"Times New Roman\", color=\"lightblue\")\n",
    "\n",
    "\n",
    "#my_name = TextBox(app, width=\"fill\", height=\"15\")\n",
    "\n",
    "get_name = PushButton(app, command=my_name, text=\"Click to record your name\")\n",
    "\n",
    "need_help = Text(app, text=\"What would you like to nag me about?\", size=30, font=\"Times New Roman\", color=\"red\")\n",
    "#nag =  TextBox(app, width=\"fill\", height=\"15\")\n",
    "\n",
    "\n",
    "\n",
    "submit_sentence = PushButton(app, text=\"record nag\", pady=20,\n",
    "    padx=10, command = submit)\n",
    "\n",
    "\n",
    "app.display()\n",
    "\n",
    "\n",
    " \n",
    "''' \n",
    "window = Window(app, title=\"2nd window\")\n",
    "picture1 = Picture(window, image=pic, grid=[0,0])\n",
    "window.hide()\n",
    " \n",
    "open_button = PushButton(app, text=\"Open\", command=open_window)\n",
    "close_button = PushButton(window, text=\"Close\", command=close_window)\n",
    " \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# THis is the code to generate the Skype chat context. I am leaving it here for later, if we want to improve it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = json.load(open('data\\messages.json', 'r', encoding=\"utf8\"))\n",
    "\n",
    "myjson = 'data\\messages.json'\n",
    "\n",
    "skype= []\n",
    "\n",
    "import json\n",
    "def get_all(myjson, key):\n",
    "    if type(myjson) == str:\n",
    "        myjson = json.load(open(myjson, 'r', encoding=\"utf8\"))\n",
    "    if type(myjson) is dict:\n",
    "        for jsonkey in myjson:\n",
    "            if type(myjson[jsonkey]) in (list, dict):\n",
    "                get_all(myjson[jsonkey], key)\n",
    "            elif jsonkey == key:\n",
    "                skype.append(myjson[jsonkey])\n",
    "    elif type(myjson) is list:\n",
    "        for item in myjson:\n",
    "            if type(item) in (list, dict):\n",
    "                get_all(item, key)\n",
    "                \n",
    "get_all(myjson, \"content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "skype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "exclude_all = [\"partlist\", \"URIObject\", \"Call Log\", \"<topicupdate>\", \"</e_m>\"]\n",
    "\n",
    "skype_clean = []\n",
    "for word in exclude_all:\n",
    "    skype_clean =[item for item in skype if word not in item]\n",
    "    skype = skype_clean\n",
    "'''    \n",
    "#The above code must be improved, this is not efficient on large scale. but works for now:\n",
    "\n",
    "def clean_skype(chat_history):\n",
    "    skype_clean2 = []\n",
    "    skype_clean2 =[item for item in chat_history if word in exclude_all not in item]\n",
    "    skype = skype_clean2\n",
    "    return skype_clean2\n",
    "    \n",
    "'''\n",
    "\n",
    "skype_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skype_clean\n",
    "context = \"\".join([i for i in skype_clean])\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this with actual text data:\n",
    "\n",
    "\n",
    "import regex as re\n",
    "# function to preprocess speech\n",
    "\n",
    "def clean_easy(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "#or:\n",
    "def clean(text):\n",
    "    \n",
    "    # removing paragraph numbers\n",
    "    text = re.sub('[0-9]+.\\t','',str(text))\n",
    "    # removing new line characters\n",
    "    text = re.sub('\\n ','',str(text))\n",
    "    text = re.sub('\\n',' ',str(text))\n",
    "    #text = re.sub('</e_m>',' ',str(text))\n",
    "    \n",
    "    # removing apostrophes\n",
    "    text = re.sub(\"'s\",'',str(text))\n",
    "    # removing hyphens\n",
    "    text = re.sub(\"-\",' ',str(text))\n",
    "    text = re.sub(\"— \",'',str(text))\n",
    "    # removing quotation marks\n",
    "    text = re.sub('\\\"','',str(text))\n",
    "    # removing salutations\n",
    "    text = re.sub(\"Mr\\.\",'Mr',str(text))\n",
    "    text = re.sub(\"Mrs\\.\",'Mrs',str(text))\n",
    "    # removing any reference to outside text\n",
    "    text = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", str(text))\n",
    "    # replacing emojis\n",
    "    text = re.sub('\\<ss type=like></ss>','good', str(text))\n",
    "    text = re.sub('\\<ss type=smile>:\\)</ss>','smiling',str(text))\n",
    "    text = re.sub('<ss type=sad>:</ss>','sad',str(text))\n",
    "    text = re.sub('<ss type=sad>: </ss>','sad',str(text))\n",
    "    text = re.sub('<ss type=\"heart\">(heart)</ss>','love',str(text))\n",
    "    text = re.sub('<b raw_pre=* raw_post=*>sarcasam</b>','evil',str(text))\n",
    "    \n",
    "    \n",
    "    return text\n",
    "\n",
    "# preprocessing speeches\n",
    "context = clean(context)\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('skype_chat.pkl', 'wb') as f:\n",
    "    pickle.dump(context, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(len(skype_clean))\n",
    "      \n",
    "def split_chat(chat):\n",
    "    questions = []\n",
    "    answers = []\n",
    "    if len(chat)%2 != 0:\n",
    "        chat.pop()\n",
    "    \n",
    "    for i in range(len(chat)):\n",
    "        if i%2 == 0:\n",
    "            questions.append(chat[i])\n",
    "        else:\n",
    "            answers.append(chat[i])\n",
    "    table = pd.DataFrame()\n",
    "    table['questions'] = questions\n",
    "    table['answers'] = answers\n",
    "    return table\n",
    "\n",
    "skype_table = split_chat(skype_clean)\n",
    "skype_table.sample(frac =1)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#use this with actual text data:\n",
    "\n",
    "\n",
    "import regex as re\n",
    "# function to preprocess speech\n",
    "\n",
    "def clean_easy(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "#or:\n",
    "def clean(text):\n",
    "    \n",
    "    # removing paragraph numbers\n",
    "    text = re.sub('[0-9]+.\\t','',str(text))\n",
    "    # removing new line characters\n",
    "    text = re.sub('\\n ','',str(text))\n",
    "    text = re.sub('\\n',' ',str(text))\n",
    "    #text = re.sub('</e_m>',' ',str(text))\n",
    "    \n",
    "    # removing apostrophes\n",
    "    text = re.sub(\"'s\",'',str(text))\n",
    "    # removing hyphens\n",
    "    text = re.sub(\"-\",' ',str(text))\n",
    "    text = re.sub(\"— \",'',str(text))\n",
    "    # removing quotation marks\n",
    "    text = re.sub('\\\"','',str(text))\n",
    "    # removing salutations\n",
    "    text = re.sub(\"Mr\\.\",'Mr',str(text))\n",
    "    text = re.sub(\"Mrs\\.\",'Mrs',str(text))\n",
    "    # removing any reference to outside text\n",
    "    text = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", str(text))\n",
    "    # replacing emojis\n",
    "    text = re.sub('\\<ss type=like></ss>','good', str(text))\n",
    "    text = re.sub('\\<ss type=smile>:\\)</ss>','smiling',str(text))\n",
    "    text = re.sub('<ss type=sad>:</ss>','sad',str(text))\n",
    "    text = re.sub('<ss type=\"heart\">(heart)</ss>','love',str(text))\n",
    "    \n",
    "    \n",
    "    \n",
    "    return text\n",
    "\n",
    "# preprocessing speeches\n",
    "skype_clean = clean(skype_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "# Create an nlp object\n",
    "doc = nlp(\"He went to play basketball\")\n",
    "\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.disable_pipes('tagger', 'parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterate over the tokens\n",
    "for token in doc:\n",
    "    # Print the token and its part-of-speech tag\n",
    "    print(token.text, \"-->\", token.pos_)\n",
    "    \n",
    "print(spacy.explain(\"PART\"))\n",
    "\n",
    "# dependency parsing\n",
    "for token in doc:\n",
    "    print(token.text, \"-->\", token.dep_)\n",
    "    \n",
    "print(spacy.explain(\"nsubj\"), spacy.explain(\"ROOT\"), spacy.explain(\"aux\"), spacy.explain(\"advcl\"), spacy.explain(\"dobj\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Indians spent over $71 billion on clothes in 2018\")\n",
    " \n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain(\"NORP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spaCy Matcher\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Initialize the matcher with the spaCy vocabulary\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc = nlp(\"Some people start their day with lemon water\")\n",
    "\n",
    "# Define rule\n",
    "pattern = [{'TEXT': 'lemon'}, {'TEXT': 'water'}]\n",
    "\n",
    "# Add rule\n",
    "matcher.add('rule_1', None, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = matcher(doc)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract matched text\n",
    "for match_id, start, end in matches:\n",
    "    # Get the matched span\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(\"You read this book\")\n",
    "doc2 = nlp(\"I will book my ticket\")\n",
    "\n",
    "pattern = [{'TEXT': 'book', 'POS': 'NOUN'}]\n",
    "\n",
    "# Initialize the matcher with the shared vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('rule_2', None, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "# make a sentence\n",
    "sentence = Sentence('I love Berlin .')\n",
    "\n",
    "# load the NER tagger\n",
    "tagger = SequenceTagger.load('ner')\n",
    "\n",
    "# run NER over sentence\n",
    "tagger.predict(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
