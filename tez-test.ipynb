{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP7tGAhpyMVwO26VkJ+7EeD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sanardi/AI-Machine-Learning/blob/master/tez-test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0u1_fBKlW3-",
        "outputId": "956b11c8-aad6-4b3b-f77b-1d95ef4b7ca8"
      },
      "source": [
        "pip install tez"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tez in /usr/local/lib/python3.6/dist-packages (0.0.7)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tez) (1.7.0+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->tez) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->tez) (0.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->tez) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->tez) (1.19.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMoM4uvPleEA",
        "outputId": "a7c179c7-96da-45c9-f0d9-5b7271aa0520"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive/', force_remount=True)\r\n",
        "\r\n",
        "!ls \"/content/drive/My Drive/Colab Notebooks\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n",
            " ALL_cars18112020.csv\n",
            " clean_sample.csv\n",
            "'Copy of 01_text-classification.ipynb'\n",
            "'Copy of Copy of 01_text-classification.ipynb'\n",
            "'Copy of europcar.csv'\n",
            " diabetes_classifier_keras\n",
            " ecode_clean_for_ml.csv\n",
            " ecode_predictor-part1.ipynb\n",
            "'ecode_predictor-part2 (1).ipynb'\n",
            " ecode_predictor-part2.ipynb\n",
            " europcar.csv\n",
            "'keras_sentiment_analysis (1).ipynb'\n",
            " keras_sentiment_analysis.ipynb\n",
            " Literature_AI.ipynb\n",
            " merged_ecode3.csv\n",
            " mount_google_drive_in_colab.ipynb\n",
            " multiclass_classifier_keras.ipynb\n",
            "'predict Filhos.ipynb'\n",
            " pytorch_transformers.ipynb\n",
            " renault_top.xlsx\n",
            " scielo\n",
            " test_df_clean.csv\n",
            " test_df.csv\n",
            " TOP50all.xlsx\n",
            " top50ecode.xlsx\n",
            " top50.xlsx\n",
            " transformers_for_spam.ipynb\n",
            " transformers.ipynb\n",
            " Untitled\n",
            " Untitled0.ipynb\n",
            " Untitled1.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "lgIsOsRrwBnA",
        "outputId": "3b29c6cb-7f5c-4e09-d166-e80ea59e5509"
      },
      "source": [
        "test_df = pd.read_csv(r\"/content/drive/My Drive/Colab Notebooks/test_df_clean.csv\")\r\n",
        "test_df"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-50b5b1dc4023>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"/content/drive/My Drive/Colab Notebooks/test_df_clean.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MC7Fon38lYbQ"
      },
      "source": [
        "import pandas as pd\r\n",
        "import tez\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import transformers\r\n",
        "from sklearn import metrics, model_selection, preprocessing\r\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\r\n",
        "\r\n",
        "\r\n",
        "class BERTDataset:\r\n",
        "    def __init__(self, text, target):\r\n",
        "        self.text = text\r\n",
        "        self.target = target\r\n",
        "        self.tokenizer = transformers.BertTokenizer.from_pretrained(\r\n",
        "            \"bert-base-uncased\", do_lower_case=True\r\n",
        "        )\r\n",
        "        self.max_len = 64\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.text)\r\n",
        "\r\n",
        "    def __getitem__(self, item):\r\n",
        "        text = str(self.text[item])\r\n",
        "        text = \" \".join(text.split())\r\n",
        "\r\n",
        "        inputs = self.tokenizer.encode_plus(\r\n",
        "            text,\r\n",
        "            None,\r\n",
        "            add_special_tokens=True,\r\n",
        "            max_length=self.max_len,\r\n",
        "            padding=\"max_length\",\r\n",
        "            truncation=True,\r\n",
        "        )\r\n",
        "\r\n",
        "        ids = inputs[\"input_ids\"]\r\n",
        "        mask = inputs[\"attention_mask\"]\r\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\r\n",
        "\r\n",
        "        return {\r\n",
        "            \"ids\": torch.tensor(ids, dtype=torch.long),\r\n",
        "            \"mask\": torch.tensor(mask, dtype=torch.long),\r\n",
        "            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\r\n",
        "            \"targets\": torch.tensor(self.target[item], dtype=torch.long),\r\n",
        "        }\r\n",
        "\r\n",
        "\r\n",
        "class BERTBaseUncased(tez.Model):\r\n",
        "    def __init__(self, num_train_steps, num_classes):\r\n",
        "        super().__init__()\r\n",
        "        self.tokenizer = transformers.BertTokenizer.from_pretrained(\r\n",
        "            \"bert-base-uncased\", do_lower_case=True\r\n",
        "        )\r\n",
        "        self.bert = transformers.BertModel.from_pretrained(\"bert-base-uncased\")\r\n",
        "        self.bert_drop = nn.Dropout(0.3)\r\n",
        "        self.out = nn.Linear(768, num_classes)\r\n",
        "\r\n",
        "        self.num_train_steps = num_train_steps\r\n",
        "        self.step_scheduler_after = \"batch\"\r\n",
        "\r\n",
        "    def fetch_optimizer(self):\r\n",
        "        param_optimizer = list(self.named_parameters())\r\n",
        "        no_decay = [\"bias\", \"LayerNorm.bias\"]\r\n",
        "        optimizer_parameters = [\r\n",
        "            {\r\n",
        "                \"params\": [\r\n",
        "                    p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\r\n",
        "                ],\r\n",
        "                \"weight_decay\": 0.001,\r\n",
        "            },\r\n",
        "            {\r\n",
        "                \"params\": [\r\n",
        "                    p for n, p in param_optimizer if any(nd in n for nd in no_decay)\r\n",
        "                ],\r\n",
        "                \"weight_decay\": 0.0,\r\n",
        "            },\r\n",
        "        ]\r\n",
        "        opt = AdamW(optimizer_parameters, lr=3e-5)\r\n",
        "        return opt\r\n",
        "\r\n",
        "    def fetch_scheduler(self):\r\n",
        "        sch = get_linear_schedule_with_warmup(\r\n",
        "            self.optimizer, num_warmup_steps=0, num_training_steps=self.num_train_steps\r\n",
        "        )\r\n",
        "        return sch\r\n",
        "\r\n",
        "    def loss(self, outputs, targets):\r\n",
        "        if targets is None:\r\n",
        "            return None\r\n",
        "        return nn.CrossEntropyLoss()(outputs, targets)\r\n",
        "\r\n",
        "    def monitor_metrics(self, outputs, targets):\r\n",
        "        if targets is None:\r\n",
        "            return {}\r\n",
        "        outputs = torch.argmax(outputs, dim=1).cpu().detach().numpy()\r\n",
        "        targets = targets.cpu().detach().numpy()\r\n",
        "        accuracy = metrics.accuracy_score(targets, outputs)\r\n",
        "        return {\"accuracy\": accuracy}\r\n",
        "\r\n",
        "    def forward(self, ids, mask, token_type_ids, targets=None):\r\n",
        "        _, o_2 = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)\r\n",
        "        b_o = self.bert_drop(o_2)\r\n",
        "        output = self.out(b_o)\r\n",
        "        loss = self.loss(output, targets)\r\n",
        "        acc = self.monitor_metrics(output, targets)\r\n",
        "        return output, loss, acc\r\n",
        "\r\n",
        "\r\n",
        "if __name__ == \"__main__\":\r\n",
        "    dfx = pd.read_csv(\"/home/abhishek/datasets/bbc-text.csv\", nrows=2000)\r\n",
        "    dfx = dfx.dropna().reset_index(drop=True)\r\n",
        "    lbl_enc = preprocessing.LabelEncoder()\r\n",
        "    dfx.category = lbl_enc.fit_transform(dfx.category.values)\r\n",
        "\r\n",
        "    df_train, df_valid = model_selection.train_test_split(\r\n",
        "        dfx, test_size=0.1, random_state=42, stratify=dfx.category.values\r\n",
        "    )\r\n",
        "\r\n",
        "    df_train = df_train.reset_index(drop=True)\r\n",
        "    df_valid = df_valid.reset_index(drop=True)\r\n",
        "\r\n",
        "    train_dataset = BERTDataset(\r\n",
        "        text=df_train.text.values, target=df_train.category.values\r\n",
        "    )\r\n",
        "\r\n",
        "    valid_dataset = BERTDataset(\r\n",
        "        text=df_valid.text.values, target=df_valid.category.values\r\n",
        "    )\r\n",
        "\r\n",
        "    n_train_steps = int(len(df_train) / 32 * 10)\r\n",
        "    model = BERTBaseUncased(\r\n",
        "        num_train_steps=n_train_steps, num_classes=dfx.category.nunique()\r\n",
        "    )\r\n",
        "\r\n",
        "    # model.load(\"model.bin\")\r\n",
        "    tb_logger = tez.callbacks.TensorBoardLogger(log_dir=\".logs/\")\r\n",
        "    es = tez.callbacks.EarlyStopping(monitor=\"valid_loss\", model_path=\"model.bin\")\r\n",
        "    model.fit(\r\n",
        "        train_dataset,\r\n",
        "        valid_dataset=valid_dataset,\r\n",
        "        train_bs=32,\r\n",
        "        device=\"cuda\",\r\n",
        "        epochs=3,\r\n",
        "        callbacks=[tb_logger, es],\r\n",
        "        fp16=True,\r\n",
        "    )\r\n",
        "    model.save(\"model.bin\")\r\n",
        "\r\n",
        "    preds = model.predict(valid_dataset, batch_size=16, n_jobs=-1, device=\"cuda\")\r\n",
        "    for p in preds:\r\n",
        "        print(p)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}